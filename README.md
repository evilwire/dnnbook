Practical Deep Neural Networks
==============================

Deep learning seems to be all the rage nowadays. With the
recent advances in deep learning, especially the success
stories, it is no surprise that it has garnered the kind
of success and popularity that it did. What is surprising
about the subject --- and definitely one of the factors
that contributes to its popularity --- is how accessible
the subject is to the average computer scientist. One
can go very very far with relatively little background
in probability and statistics, multivariate calculus,
and algorithms.

The idea of machine learning is very simple that it can
be summarized by a singe sentence: finding the right
function to optimize. This may sound easier than we
make it out to be. In practice, this is all the
challenge. Machine learning wouldn't be where it is
today --- and the computer system wouldn't be where
it is --- if it was simple.

Machine learning is still very nascent. There are great
mathematical potentials to be had, and in essence, the
field has reached a tipping point where the statistical
algorithms is reaching a saturation. While there are
still many more statistical techniques in optimizing that still lay
dormant, there is still the vast expanse of algebraic
techniques that remain untapped. Fields like Conley Theory
that use algebraic methods to gauge the correctness
of learned models from the perspective of stability in 
a dynamical systems remains theoretical. While deep
learning is definitely in the foreground as a dominant
player in artificial intelligence, I am very excited
about the next revolution.

## Who Are We?

### Knight Fu

I am a recent graduate in math PhD from Rutgers 
University. While my chief interest is in algebraic
k-theory and motivic cohomology, I have always had
an interest in the union (and intersection) of 
algorithms and machine learning. Mathematics has
always contributed to real-world problem solving,
and with computer science, the more theoretical
elements of mathematics have really shined.
